#Load in train and test data
data <- read.csv("C:/Users/Roopa Ravishankar/Downloads/train.csv")
data <- data %>% select(-c("Name", "PassengerId", "Cabin"))
data <- data %>% transform(HomePlanet = as.factor(HomePlanet),
                           CryoSleep = as.factor(CryoSleep),
                           Destination = as.factor(Destination), 
                           VIP = as.factor(VIP), 
                           Transported = as.logical(Transported)
)
data$Transported <- 1*data$Transported
data <- na.omit(data)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))

#split into Training and Testing
train <- data[sample, ]
test <- data[!sample, ]

#Bagged Random Forest
#Write a loop to figure out ideal number of trees
MSEtree<- data.frame("Trees"= 1, "Accuracy" = 0, "i" = 0)
n=10
for (i in 10:500){
  baggedtree<- randomForest(Transported~., train, mtry = ncol(train)-1,n.tree=i, importance = TRUE)
  pred_baggedtree<- predict(baggedtree, test)
  pred_baggedtree <- 1 * (pred_baggedtree > 0.5)
  
  #Accuracy
  accuracy<-mean(pred_baggedtree == test$Transported)
  MSEtree<- MSEtree%>% add_row(Trees = n, Accuracy = accuracy, i = i)
  n=n+1
}

MSEtree%>%ggplot(aes(x = Trees, y = Accuracy)) + 
  geom_point(size = .75, alpha = .6) +
  geom_line(color = "skyblue") +
  ggtitle("Accuracy based on Number of Trees")
Accuracy <- data.frame("Model" = "Bagging", "Accuracy" = MSEtree[which.min(MSEtree$Accuracy),2])

The optimal number of trees is `r MSEtree[which.min(MSEtree$Accuracy),3]`

#Boosted Random Forest
set.seed(1)
Shrink.MSE<- data.frame("ShrinkageValue" = NA, "Accuracy" = NA)

for (i in 1:500){
  #Fit Boosted
  n=i/10000
  boosteddata <- gbm(Transported~., data = train, distribution ="bernoulli", shrinkage = n, n.trees = MSEtree[which.min(MSEtree$Accuracy),3], interaction.depth = 4)
  
  #Predict on Training
  pred.boosted <-predict(boosteddata, test, n.trees=1000)
  pred.boosted <- 1 * (pred.boosted > 0.5)
  
  #MSE
  accuracy<-mean(as.factor(pred.boosted) == test$Transported)
  Shrink.MSE<- Shrink.MSE%>%add_row(ShrinkageValue = n, Accuracy = accuracy)
}

Shrink.MSE%>%ggplot(aes(x = ShrinkageValue, y = Accuracy)) + 
  geom_point(size = .75, alpha = .6) +
  geom_line(color = "skyblue") +
  ggtitle("Accuracy based on Shrinkage Value")
Accuracy <- Accuracy %>% add_row("Model" = "Boosting", "Accuracy" = Shrink.MSE[which.min(Shrink.MSE$Accuracy),2])

So, the optimal shrinkage parameter is `r Shrink.MSE[which.min(Shrink.MSE$MSE),1]` with a test MSE of `r Shrink.MSE[which.min(Shrink.MSE$MSE),2]`.

#Logistic
lin.mod<-glm(Transported ~., data = train)
pred.lin.mod<-predict(lin.mod,  test)
pred.lin.mod <- 1 * (pred.lin.mod > 0.5)

#MSE
accuracy<-mean(as.factor(pred.lin.mod) == test$Transported)
Accuracy <- Accuracy %>% add_row(Model = "Logistic", Accuracy = accuracy)


#Logistic
lin.mod<-lm(Transported ~., data = train)
pred.lin.mod<-predict(lin.mod,  test)
pred.lin.mod <- 1 * (pred.lin.mod > 0.5)

#MSE
accuracy<-mean(as.factor(pred.lin.mod) == test$Transported)
Accuracy <- Accuracy %>% add_row(Model = "Linear", Accuracy = accuracy)

print(Accuracy)


Based on the table, the best model is given by `r Accuracy[which.min(Accuracy$Accuracy),1]`. Note that we have also optimized the Boosting and Bagging models with the optimal number of trees and shrinkage parameter. We might get an even better accuracy if we use LASSO or Ridge, but it seems we are doing okay right now. 

